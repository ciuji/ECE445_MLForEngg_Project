{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Friday--Cluster\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "\"Dataset of 550 000 observations about the black Friday in a retail store, it contains different kinds of variables either numerical or categorical. It contains missing values.\"\n",
    "\n",
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        User_ID Product_ID Gender    Age  Occupation City_Category  \\\n",
      "21390   1003382  P00332342      F   0-17          10             A   \n",
      "147892  1004808  P00117542      M  36-45           0             A   \n",
      "172461  1002676  P00026042      M    55+          20             B   \n",
      "389880  1006001  P00215742      F  26-35           7             A   \n",
      "533742  1004160  P00114242      M  36-45          20             A   \n",
      "\n",
      "       Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
      "21390                           2               0                   5   \n",
      "147892                          1               1                  18   \n",
      "172461                          1               1                   8   \n",
      "389880                          0               1                   5   \n",
      "533742                         4+               1                  13   \n",
      "\n",
      "        Product_Category_2  Product_Category_3  Purchase  \n",
      "21390                  8.0                 NaN      7002  \n",
      "147892                 NaN                 NaN      3074  \n",
      "172461                14.0                17.0      8043  \n",
      "389880                16.0                 NaN      5183  \n",
      "533742                15.0                 NaN       584  \n"
     ]
    }
   ],
   "source": [
    "df_allData=pd.read_csv('BlackFriday.csv')\n",
    "print(df_allData.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw data has 12 columns and there are a lot of missing values in \"Product_Category_2\" and \"Product_Category_3\". Which means some of the product whould just have one category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our opinion, we think our model would do cluster on different people, so our key is people.\n",
    "\n",
    "But the dataset includes different records of one-single people. So at the beginning we use gruopby to get each people's whole records. \n",
    "\n",
    "Then we try to get the mode of each one's \"Product_Category_1\" to represent the main product category and get the mean of one people's whole \"Purchase\" as a feature of \"average purchase\". (We are not sure weather this is a good way buy we have to do this because we cannot keep all the data to train)\n",
    "\n",
    "What's more, we change the \"Gender\" attribute to 0-1 attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Changlin Jiang\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Occupation    Age City_Category  Marital_Status  Product_CateGory_1  \\\n",
      "User_ID                                                                        \n",
      "1002479           1  26-35             C               0                   1   \n",
      "1001835          19  26-35             B               0                   5   \n",
      "1002234           1    55+             C               1                   8   \n",
      "1005886          20  26-35             A               0                   1   \n",
      "1003454           0  18-25             A               1                   1   \n",
      "\n",
      "        Stay_In_Current_City_Years  times  Gender_M      Purchase  \n",
      "User_ID                                                            \n",
      "1002479                          2     19         0  10839.263158  \n",
      "1001835                          3    435         1  10162.694253  \n",
      "1002234                          0     12         1  11402.083333  \n",
      "1005886                          2    255         1   9082.898039  \n",
      "1003454                          1    198         1   9546.297980  \n"
     ]
    }
   ],
   "source": [
    "groupByUserData=df_allData.groupby(['User_ID'])\n",
    "\n",
    "times=df_allData['User_ID'].value_counts()\n",
    "times=times.sort_index()\n",
    "\n",
    "#get the mean\n",
    "meanData=groupByUserData.mean()\n",
    "\n",
    "#get the mode\n",
    "modeData=groupByUserData.agg(lambda x: stats.mode(x)[0][0])\n",
    "\n",
    "mean_mode_data={'Gender':modeData['Gender'],'Occupation':modeData['Occupation'],'Age':modeData['Age'],'City_Category':modeData['City_Category'],'Marital_Status':modeData['Marital_Status'],'Product_CateGory_1':modeData['Product_Category_1'],'Stay_In_Current_City_Years':modeData['Stay_In_Current_City_Years']}\n",
    "mean_mode_data=pd.DataFrame(mean_mode_data)\n",
    "mean_mode_data['times']=times\n",
    "mean_mode_data['Gender_M']=pd.get_dummies(mean_mode_data['Gender'])['M']\n",
    "mean_mode_data=mean_mode_data.drop(['Gender'],axis=1)\n",
    "mean_mode_data['Purchase']=meanData['Purchase']\n",
    "\n",
    "print (mean_mode_data.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the hardest part of our cluster project.\n",
    "\n",
    "There are two key problems we have to face:\n",
    "\n",
    "##### 1. **How to handle the discrete attributes?**\n",
    "    \n",
    "    There are a lot of disordered discrete attributes in our data, likes \"Marital_Status\", \"Gender\" and \"Product_Category_1\", we cannot just simplely calculate their euclidean distance.\n",
    "\n",
    "##### 2. **How to evaluate our feature extraction performance?**\n",
    "\n",
    "    Since we have so many choices of extracting the features and we do not know how to assign the weights on these feautres and this is a cluster problem, we had not a clearly mind of have to evaluate our work when we doing the feature extraction. And it is unrealistic to try all the choices and train them then evaluate the final models. The best solution is that we can find some explainable output of our feature extraction.\n",
    "    \n",
    "#### Deal with discrete attributes\n",
    "\n",
    "To solve the key problems 1, we came up with ideas.\n",
    "\n",
    "1. do one-hot encoding on the discrete attributes\n",
    "\n",
    "1. use k-modes or k-prototype model\n",
    "\n",
    "1. drop those discrete attributes\n",
    "\n",
    "But their would raise some new problems if we use these solutions:\n",
    "\n",
    "    one-hot encoding would make the features very sparse.\n",
    "    \n",
    "    It is not easy to combine the VDM distance and Minkowski distance together.\n",
    "    \n",
    "    Some discrete features may be very important like occupation but it would have a lot of possible value.\n",
    "    \n",
    "#### Deal with evaluation\n",
    "\n",
    "We do not find a good way to solve this problem, we just living with it, but we still did some tries. Likes using the average purchase to evaluate the cluster output. And we use Calinski-Harabasz score and Value Difference Metrix to evalue our final clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing Before Training\n",
    "\n",
    "Since we have features of different units, we must do the data standardization and assign different weights to different features.\n",
    "\n",
    "We choose min-max standardization and assign the weights by feeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Model 1: K-modes by Chaoji Zuo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I try to use the k-modes only on the categorical values to train a model, then use one-hot encoidng data to calculate the jaccard distance and euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kmodes import KModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical data:\n",
      "        Gender  Occupation    Age City_Category  Marital_Status  \\\n",
      "User_ID                                                           \n",
      "1005051      F           7  51-55             C               0   \n",
      "1005769      M          14  46-50             B               1   \n",
      "\n",
      "         Product_CateGory_1 Stay_In_Current_City_Years  \n",
      "User_ID                                                 \n",
      "1005051                   5                          0  \n",
      "1005769                   1                          1  \n",
      "one-hot encoding data:\n",
      "         0-17  18-25  26-35  36-45  46-50  51-55  55+  A  B  C  \\\n",
      "User_ID                                                          \n",
      "1002209     0      0      0      1      0      0    0  1  0  0   \n",
      "1000982     0      0      1      0      0      0    0  0  0  1   \n",
      "\n",
      "              ...        8  10  11  12  13  15  16  18  Gender_M  \\\n",
      "User_ID       ...                                                  \n",
      "1002209       ...        0   0   0   0   0   0   0   0         1   \n",
      "1000982       ...        1   0   0   0   0   0   0   0         0   \n",
      "\n",
      "         Marital_Status  \n",
      "User_ID                  \n",
      "1002209               1  \n",
      "1000982               0  \n",
      "\n",
      "[2 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "X=pd.DataFrame({'Gender':modeData['Gender'],'Occupation':modeData['Occupation'],'Age':modeData['Age'],'City_Category':modeData['City_Category'],'Marital_Status':modeData['Marital_Status'],'Product_CateGory_1':modeData['Product_Category_1'],\"Stay_In_Current_City_Years\":modeData[\"Stay_In_Current_City_Years\"]})\n",
    "\n",
    "one_hot_city=pd.get_dummies(mean_mode_data['City_Category'])\n",
    "one_hot_age=pd.get_dummies(mean_mode_data['Age'])\n",
    "one_hot_occupation=pd.get_dummies(mean_mode_data['Occupation'])\n",
    "one_hot_years=pd.get_dummies(mean_mode_data['Stay_In_Current_City_Years'])\n",
    "one_hot_product=pd.get_dummies(mean_mode_data['Product_CateGory_1'])\n",
    "XX=pd.concat([one_hot_age,one_hot_city,one_hot_occupation,one_hot_years,one_hot_product],axis=1)\n",
    "XX['Gender_M']=mean_mode_data['Gender_M']\n",
    "XX['Marital_Status']=mean_mode_data['Marital_Status']\n",
    "\n",
    "print (\"categorical data:\")\n",
    "print(X.sample(2))\n",
    "print(\"one-hot encoding data:\")\n",
    "print(XX.sample(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KModes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3882f74f1032>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mjcArr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mkm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKModes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtempArr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KModes' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "jcArr=[]\n",
    "for i in range(2,10):\n",
    "    km=KModes(n_clusters=i)\n",
    "    y=km.fit_predict(X)\n",
    "    tempArr=[]\n",
    "    for j in range(i):\n",
    "        #print(sum(y==j))\n",
    "        #print(XX[y==j].mode())\n",
    "        jcscore=[]\n",
    "        for k in XX[y==j].T:\n",
    "            try:\n",
    "                jcscore.append(jaccard_similarity_score(XX.loc[k],XX[y==j].mode().T[0]))\n",
    "            except:\n",
    "                #print(XX.loc[k].T)\n",
    "                print(XX[y==j].mode())\n",
    "                hhh=XX[y==j].mode()\n",
    "                #print(k)\n",
    "                break;\n",
    "        #print(np.mean(jcscore))\n",
    "        tempArr.append(np.mean(jcscore))\n",
    "    print(\"n_cluster =\",i,\":\",np.mean(tempArr))\n",
    "    jcArr.append(np.mean(tempArr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecArr=[]\n",
    "for i in range(2,10):\n",
    "    km=KModes(n_clusters=i)\n",
    "    y=km.fit_predict(X)\n",
    "    tempArr=[]\n",
    "    for j in range(i):\n",
    "        #print(sum(y==j))\n",
    "        #print(XX[y==j].mode())\n",
    "        ecscore=[]\n",
    "        for k in XX[y==j].T:\n",
    "            if(k):\n",
    "                ecscore.append(np.linalg.norm(np.array(XX.loc[k])-np.array(XX[y==j].mode().T[0])))\n",
    "        #print(np.mean(ecscore))\n",
    "        tempArr.append(np.mean(ecscore))\n",
    "    print(\"n_cluster =\",i,\":\",np.mean(tempArr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(np.array(XX.loc[1004491])-np.array(XX.mode().T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Model 2: XXX by Shuyu Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Model 3: ROCK Clustering by Changlin Jiang\n",
    "- After searching relavant paper dealing with discrete features clustering,we decide to implement a algorithm termed ROCK.  \n",
    "- The main idea is:  \n",
    "    - Compute the Jaccard score between 2 data points.Input a threshold theta such that if the Jaccard score is larger than the threshold,say these 2 points are neighbors (i.e. they have a Neighbor label which is originally 0 and can be set to 1 if their Jaccard score is larger than the threshold theta.)  \n",
    "    - Compute the Neighbor label of every 2 data,put them in a Neighbors Matrix A,A[i,j]=1 if point i and j are neighbors,A[i,j]=0 if not.\n",
    "    - Initialization:If note the number of data points with n,initialize the clusters with n clusters,each cluster includes only one data point.\n",
    "    - Gooodness:The goodness of 2 clusters $C_i$ and $C_j$ can be determined by: $$goodness=\\frac{link(C_i,C_j)}{(n1+n2)^{1+2f(\\theta)}-n1^{1+2f(\\theta)}-n2^{1+2f(\\theta)}}$$In this formula,link(Ci,Cj) is the total number of neighbors of the two clusters $C_i$ and $C_j$,$$f(\\theta)=\\frac{1-\\theta}{1+\\theta}$$is the penalty if the size of the cluster has got too many members.\n",
    "    - Compute goodness of every 2 different clusters,find the pair of clusters with the largest goodness.\n",
    "    - Process:Merge the two clusters into one clusters and loop the whole process untill converge or number of clusters is smaller than a wanted number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 10 0 ... 0 3 2]\n",
      " [0 16 6 ... 0 1 4]\n",
      " [0 15 2 ... 0 1 3]\n",
      " ...\n",
      " [0 7 2 ... 1 5 1]\n",
      " [0 0 2 ... 0 10 1]\n",
      " [1 0 1 ... 1 11 3]]\n"
     ]
    }
   ],
   "source": [
    "X=np.array(X)\n",
    "X0=list(X[:,0])\n",
    "X2=list(X[:,2])\n",
    "X3=list(X[:,3])\n",
    "X6=list(X[:,6])\n",
    "\n",
    "X0=[0 if x=='M' else 1 for x in X0]\n",
    "\n",
    "X2=[0 if x=='0-17' else x for x in X2]\n",
    "X2=[1 if x=='18-25' else x for x in X2]\n",
    "X2=[2 if x=='26-35' else x for x in X2]\n",
    "X2=[3 if x=='36-45' else x for x in X2]\n",
    "X2=[4 if x=='46-50' else x for x in X2]\n",
    "X2=[5 if x=='51-55' else x for x in X2]\n",
    "X2=[6 if x=='55+' else x for x in X2]\n",
    "\n",
    "X3=[0 if x=='A' else x for x in X3]\n",
    "X3=[1 if x=='B' else x for x in X3]\n",
    "X3=[2 if x=='C' else x for x in X3]\n",
    "\n",
    "X6=[0 if x=='0' else x for x in X6]\n",
    "X6=[1 if x=='1' else x for x in X6]\n",
    "X6=[2 if x=='2' else x for x in X6]\n",
    "X6=[3 if x=='3' else x for x in X6]\n",
    "X6=[4 if x=='4+' else x for x in X6]\n",
    "\n",
    "X[:,0]=np.array(X0).T\n",
    "X[:,2]=np.array(X2).T\n",
    "X[:,3]=np.array(X3).T\n",
    "X[:,6]=np.array(X6).T\n",
    "\n",
    "X1=X[:300,:]\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive neighbors matrix A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "def neighbors_matrix(data,theta):\n",
    "    m,n=np.shape(data)\n",
    "    A=np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if jaccard_similarity_score(np.reshape(list(data[i,:]),-1),np.reshape(list(data[j,:]),-1))>=theta:\n",
    "                A[i,j]=1\n",
    "    return A.astype(int)\n",
    "\n",
    "A=neighbors_matrix(X1,0.5)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the $link(C_i,C_j)$ of $C_i$ and $C_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link(A,c1,c2):\n",
    "    m,n=np.shape(A)\n",
    "    sum_=0\n",
    "    if isinstance(c1,int)==True:\n",
    "        n1=1\n",
    "    else:\n",
    "        n1=len(c1)\n",
    "    if isinstance(c2,int)==True:\n",
    "        n2=1\n",
    "    else:\n",
    "        n2=len(c2)\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            if isinstance(c1,int)==True:\n",
    "                k1=c1\n",
    "            else:\n",
    "                k1=c1[i]\n",
    "            if isinstance(c2,int)==True:\n",
    "                k2=c2\n",
    "            else:\n",
    "                k2=c2[j]\n",
    "            sum_+=A[k1,k2]\n",
    "    return sum_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute goodness of $C_i$ and $C_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodness(A,c1,c2,theta):\n",
    "    if isinstance(c1,int)==True:\n",
    "        n1=1\n",
    "    else:\n",
    "        n1=len(c1)\n",
    "    if isinstance(c2,int)==True:\n",
    "        n2=1\n",
    "    else:\n",
    "        n2=len(c2)\n",
    "    ftheta=(1-theta)/(1+theta)\n",
    "    if c1==c2:\n",
    "        gm=0\n",
    "    else:\n",
    "        gm=link(A,c1,c2)/((n1+n2)**(1+2*ftheta)-n1**(1+2*ftheta)-n2**(1+2*ftheta))\n",
    "    return gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_pair(A,clusters,theta):\n",
    "    maximum_goodness = 0.0;\n",
    "    cluster_indexes = [-1, -1];\n",
    "        \n",
    "    for i in range(0, len(clusters)):\n",
    "        for j in range(i + 1, len(clusters)):\n",
    "            gm = goodness(A,clusters[i], clusters[j],theta);\n",
    "            if (gm > maximum_goodness):\n",
    "                maximum_goodness = gm;\n",
    "                cluster_indexes = [i, j]\n",
    "        \n",
    "    return cluster_indexes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(clusters,cluster_indexes):\n",
    "    if isinstance(clusters[cluster_indexes[0]],int)==True:\n",
    "        clusters[cluster_indexes[0]]=[clusters[cluster_indexes[0]]]\n",
    "    if isinstance(clusters[cluster_indexes[1]],int)==True:\n",
    "        clusters[cluster_indexes[1]]=[clusters[cluster_indexes[1]]]\n",
    "    clusters[cluster_indexes[0]].extend(clusters[cluster_indexes[1]])\n",
    "    clusters.pop(cluster_indexes[1])\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and clustering result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters<=10\n",
      "[[0, 18, 83, 190, 206, 48, 72, 96, 115, 149, 196, 5, 10, 36, 66, 197, 215, 103, 163, 192, 244, 272, 111, 135], [1, 13, 16, 24, 28, 49, 58, 63, 81, 82, 84, 97, 110, 114, 116, 117, 125, 139, 170, 177, 181, 184, 202, 221, 247, 251, 255, 274, 289, 175, 238, 278, 283, 91, 128, 140, 14, 218, 226, 257, 258, 80, 166, 223, 161, 267, 19, 252, 256, 159, 213, 70, 122, 185, 188, 195, 107, 2, 231, 55, 143, 118, 209, 266, 60, 210, 224, 74, 178, 38, 158, 89, 98, 127, 141, 241, 54, 8, 245, 240, 131, 187, 172, 263, 225, 295, 296, 37, 67, 65, 148, 198, 186, 234, 269, 292, 11, 237, 30, 87, 108, 271, 182, 168, 298, 20, 35, 132, 222, 230, 165, 205, 212, 219, 254, 40, 105, 208, 216, 138, 21, 33, 59, 153, 201, 193, 220, 43, 44, 104, 176, 273, 157, 293, 249, 53, 99, 156, 229, 261, 285], [3, 6, 12, 22, 41, 45, 100, 121, 126, 144, 145, 173, 279, 73, 90, 200, 243, 259, 265, 268, 294, 79, 112, 113, 130, 62, 207, 204, 277, 77, 102, 211, 253, 291, 57, 106, 286, 236, 250, 39, 92, 46, 235, 61, 194, 246, 297, 4, 7, 26, 120, 129, 137, 169, 232, 264, 287, 32, 124, 262, 203, 227, 78, 47, 147, 68, 86, 270, 134, 42, 69, 76, 85, 155, 123, 179, 189, 9, 15, 27, 75, 152, 34, 51, 151, 275, 88, 101, 214, 288, 142, 260, 23, 25, 239, 119, 146, 162, 191, 282], [17, 94, 136, 29, 31, 248, 281, 284, 50, 52, 171, 183, 242, 280, 160, 233], [56, 64, 167, 199], [71, 174], [93, 95, 133, 164, 109, 180], [150, 290], [154, 228], [217, 276, 299]]\n"
     ]
    }
   ],
   "source": [
    "clusters=list([i for i in range(300)])\n",
    "for i in range(len(clusters)-10):\n",
    "    new_clusters=merge(clusters,find_best_pair(A,clusters,0.5))\n",
    "    if len(new_clusters)<=10:\n",
    "        print(\"Number of clusters<=10\")\n",
    "        print(clusters)\n",
    "        break;\n",
    "    else:\n",
    "        clusters=new_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
